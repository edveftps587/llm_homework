{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "396a2787-15fd-4dde-941e-160b4b87436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82eb5f3a-554b-426c-801c-27f2517a1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca39520b-e419-481b-9c17-dc84a2f7f811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c46cdc4-6886-4d13-a1d9-cb75116b19fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dfb9eaf-b8c1-4ca2-ad32-c464b8b1ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f056bf6-8d0c-4209-990e-afa4a77de7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdee1f68-1066-4a9a-b15a-3b7ee970f4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91a382b6-d55d-4aa2-9d31-3b2594877561",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 \n",
    "doc_stride = 128 \n",
    "# 251,252 是两条条超过max_length的记录\n",
    "test_251 = datasets['train'][251]\n",
    "test_252 = datasets['train'][252]\n",
    "test_example = {\n",
    "    'id': [test_251['id'], test_252['id']],\n",
    "    'title': [test_251['title'], test_252['title']],\n",
    "    'context': [test_251['context'], test_252['context']],\n",
    "    'question': [test_251['question'], test_252['question']],\n",
    "    'answers': [test_251['answers'], test_252['answers']],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f438ce7f-451a-4635-af77-562d7292d5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['5733caf74776f4190066124e', '5733caf74776f4190066124f'],\n",
       " 'title': ['University_of_Notre_Dame', 'University_of_Notre_Dame'],\n",
       " 'context': [\"The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\",\n",
       "  \"The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\"],\n",
       " 'question': [\"How many NCAA tournaments did the Notre Dame men's basketball team take part in?\",\n",
       "  \"Which Notre Dame men's basketball player has the record for more points in one game?\"],\n",
       " 'answers': [{'text': ['28'], 'answer_start': [119]},\n",
       "  {'text': ['Austin Carr'], 'answer_start': [154]}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e9b6ce1-d0de-4b41-af84-de80ba9f5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    \"\"\"\n",
    "    整体方法是在标准的tokenizer基础上，增加了start_positions和end_positions两列，代表答案token的开始和结束位置\n",
    "    \"\"\"\n",
    "    if isinstance(examples[\"question\"], str):\n",
    "        examples[\"question\"] = examples[\"question\"].lstrip()\n",
    "    else:\n",
    "        examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "        \n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 切片后的chunk => 原文\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # token pos => char pos\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i] # chunk token\n",
    "\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id) # cls pos, always 0\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i) # token mask，0-question，1-answer，None-special token\n",
    "        \n",
    "        sample_index = sample_mapping[i] # 对应的原文\n",
    "        \n",
    "        answers = examples[\"answers\"][sample_index] # 对应原文的答案\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0: # 'answers': {'text': [''], 'answer_start': []}}，原文中没有答案\n",
    "            # 开始结束位置记录为 0\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在原文中的开始和结束位置\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0 # token对应的context开始的位置\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1 \n",
    "\n",
    "            token_end_index = len(input_ids) - 1 # token对应的context结束的位置\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1 \n",
    "            \n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                # 当前chunk中没有答案（必须完整包含）\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 当前chunk中有答案\n",
    "                # 后移token的位置，找到答案对应的token开始位置\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                # 前移token的位置，找到答案对应的token结束位置\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "228562ca-ab51-40ea-9b3c-e1a10657827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_example = prepare_train_features(test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8bc002a-61c7-4626-8830-283453d28a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93516915-82fb-4ce5-9ffd-26004c8fb892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 88524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10784\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20eea042-bfe9-4513-ad5e-b125d8321db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0dce104-ba25-452c-9ee2-16c640e7d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=192\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08ea8ba8-b9c7-4692-b3e5-5f675c2c7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33871300-67b3-4de5-861d-20f497c11f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62867476-300f-4322-a5c0-ee2466d8f58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1386' max='1386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1386/1386 33:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.428168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.157600</td>\n",
       "      <td>1.275152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.303900</td>\n",
       "      <td>1.251565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(model_dir)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af3993d9-21aa-4ad2-80b0-3c88fa8048c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    \"\"\"\n",
    "    根据特征和预测还原到原本的答案\n",
    "    \"\"\"\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 构建一个从示例到其对应特征的映射。\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # 我们需要填充的字典。\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # 日志记录。\n",
    "    print(f\"正在后处理 {len(examples)} 个示例的预测，这些预测分散在 {len(features)} 个特征中。\")\n",
    "\n",
    "    # 遍历所有示例！\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 这些是与当前示例关联的特征的索引。\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # 仅在squad_v2为True时使用。\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # 遍历与当前示例关联的所有特征。\n",
    "        for feature_index in feature_indices:\n",
    "            # 我们获取模型对这个特征的预测。\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 这将允许我们将logits中的某些位置映射到原始上下文中的文本跨度。\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 更新最小空预测。\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 浏览所有的最佳开始和结束logits，为 `n_best_size` 个最佳选择。\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # 在极少数情况下我们没有一个非空预测，我们创建一个假预测以避免失败。\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # 选择我们的最终答案：最佳答案或空答案（仅适用于squad_v2）\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ed87036-b760-4b89-9e2b-bbd15d388b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    \"\"\"\n",
    "    将验证数据处理为features\n",
    "    \"\"\"\n",
    "    # 一些问题的左侧有很多空白，这些空白并不有用且会导致上下文截断失败（分词后的问题会占用很多空间）。\n",
    "    # 因此我们移除这些左侧空白\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和可能的填充对我们的示例进行分词，但使用步长保留溢出的令牌。这导致一个长上下文的示例可能产生\n",
    "    # 几个特征，每个特征的上下文都会稍微与前一个特征的上下文重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例在上下文很长时可能会产生几个特征，我们需要一个从特征映射到其对应示例的映射。这个键就是为了这个目的。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # 我们保留产生这个特征的示例ID，并且会存储偏移映射。\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 获取与该示例对应的序列（以了解哪些是上下文，哪些是问题）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        # 一个示例可以产生几个文本段，这里是包含该文本段的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # 将不属于上下文的偏移映射设置为None，以便容易确定一个令牌位置是否属于上下文。\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "908b6add-fe30-4dae-9bd2-574f70cad299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcedf63e7b43424cb6b2138a9b01e73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca15fb5d-623a-4c42-bf30-bc72dc5504b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 10784\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0b17eb9-5228-4052-9f9a-140cc094bab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)\n",
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 基于featture的原始预测结果\n",
    "raw_predictions = trained_trainer.predict(validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a0f503b-6b31-45ee-b712-3eb7ff54c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-5.286222 , -6.612807 , -7.293781 , ..., -7.5817404, -7.56017  ,\n",
      "        -7.627872 ],\n",
      "       [-5.298247 , -6.6009326, -7.273103 , ..., -7.583229 , -7.5616307,\n",
      "        -7.628161 ],\n",
      "       [-2.720223 , -5.06894  , -6.461087 , ..., -7.641131 , -7.598675 ,\n",
      "        -7.589619 ],\n",
      "       ...,\n",
      "       [-2.084156 , -6.447255 , -6.801171 , ..., -7.570975 , -7.555427 ,\n",
      "        -7.5553794],\n",
      "       [-1.9123228, -6.6626377, -6.757734 , ..., -7.5725675, -7.5949917,\n",
      "        -7.569608 ],\n",
      "       [-1.7271022, -6.8095   , -6.809823 , ..., -7.6145835, -7.5947175,\n",
      "        -7.5965524]], dtype=float32), array([[-4.779095 , -7.073431 , -6.561958 , ..., -7.4892554, -7.509979 ,\n",
      "        -7.4518743],\n",
      "       [-4.8729267, -7.0950127, -6.587563 , ..., -7.4872966, -7.5079274,\n",
      "        -7.4510503],\n",
      "       [-2.4264112, -6.2001505, -7.2404637, ..., -7.432796 , -7.484364 ,\n",
      "        -7.4749546],\n",
      "       ...,\n",
      "       [-2.0083191, -7.132052 , -7.415781 , ..., -7.5316133, -7.5426645,\n",
      "        -7.58057  ],\n",
      "       [-1.8068904, -7.087704 , -7.2247133, ..., -7.5359416, -7.4681125,\n",
      "        -7.5394278],\n",
      "       [-1.5354849, -6.892689 , -7.496103 , ..., -7.5043445, -7.524335 ,\n",
      "        -7.5558014]], dtype=float32)), label_ids=None, metrics={'test_runtime': 27.1306, 'test_samples_per_second': 397.485, 'test_steps_per_second': 2.101})\n",
      "10784\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(raw_predictions)\n",
    "print(len(raw_predictions.predictions[0]))\n",
    "print(len(raw_predictions.predictions[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71c32241-8d9a-4397-a234-8503f3fcde52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 10570 个示例的预测，这些预测分散在 10784 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364658cab24f4bcba5e7bf4816fa77f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_900124/2594762423.py:10: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad\")\n",
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9c37c346b64d89846b406b8233965a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fb8fab4f9c456498ddfda5d7c7b572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 71.57048249763481, 'f1': 81.15865261806042}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# 基于原数据、特征、预测结果还原回答\n",
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)\n",
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "\n",
    "# 标准答案\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "# 计算准确率和f1 score\n",
    "metric = load_metric(\"squad\")\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ab85e-4502-48a5-9adb-c249fe3b65cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
