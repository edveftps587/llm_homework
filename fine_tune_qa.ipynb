{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "396a2787-15fd-4dde-941e-160b4b87436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82eb5f3a-554b-426c-801c-27f2517a1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca39520b-e419-481b-9c17-dc84a2f7f811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c46cdc4-6886-4d13-a1d9-cb75116b19fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dfb9eaf-b8c1-4ca2-ad32-c464b8b1ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f056bf6-8d0c-4209-990e-afa4a77de7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdee1f68-1066-4a9a-b15a-3b7ee970f4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 2088, 102], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91a382b6-d55d-4aa2-9d31-3b2594877561",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 \n",
    "doc_stride = 128 \n",
    "# 251,252 æ˜¯ä¸¤æ¡æ¡è¶…è¿‡max_lengthçš„è®°å½•\n",
    "test_251 = datasets['train'][251]\n",
    "test_252 = datasets['train'][252]\n",
    "test_example = {\n",
    "    'id': [test_251['id'], test_252['id']],\n",
    "    'title': [test_251['title'], test_252['title']],\n",
    "    'context': [test_251['context'], test_252['context']],\n",
    "    'question': [test_251['question'], test_252['question']],\n",
    "    'answers': [test_251['answers'], test_252['answers']],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f438ce7f-451a-4635-af77-562d7292d5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['5733caf74776f4190066124e', '5733caf74776f4190066124f'],\n",
       " 'title': ['University_of_Notre_Dame', 'University_of_Notre_Dame'],\n",
       " 'context': [\"The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009â€“2010 season. The team is coached by Mike Brey, who, as of the 2014â€“15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010â€“11 team concluded its regular season ranked number seven in the country, with a record of 25â€“5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\",\n",
       "  \"The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009â€“2010 season. The team is coached by Mike Brey, who, as of the 2014â€“15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010â€“11 team concluded its regular season ranked number seven in the country, with a record of 25â€“5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\"],\n",
       " 'question': [\"How many NCAA tournaments did the Notre Dame men's basketball team take part in?\",\n",
       "  \"Which Notre Dame men's basketball player has the record for more points in one game?\"],\n",
       " 'answers': [{'text': ['28'], 'answer_start': [119]},\n",
       "  {'text': ['Austin Carr'], 'answer_start': [154]}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e9b6ce1-d0de-4b41-af84-de80ba9f5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    \"\"\"\n",
    "    æ•´ä½“æ–¹æ³•æ˜¯åœ¨æ ‡å‡†çš„tokenizeråŸºç¡€ä¸Šï¼Œå¢åŠ äº†start_positionså’Œend_positionsä¸¤åˆ—ï¼Œä»£è¡¨ç­”æ¡ˆtokençš„å¼€å§‹å’Œç»“æŸä½ç½®\n",
    "    \"\"\"\n",
    "    if isinstance(examples[\"question\"], str):\n",
    "        examples[\"question\"] = examples[\"question\"].lstrip()\n",
    "    else:\n",
    "        examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "        \n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # åˆ‡ç‰‡åçš„chunk => åŸæ–‡\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # token pos => char pos\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i] # chunk token\n",
    "\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id) # cls pos, always 0\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i) # token maskï¼Œ0-questionï¼Œ1-answerï¼ŒNone-special token\n",
    "        \n",
    "        sample_index = sample_mapping[i] # å¯¹åº”çš„åŸæ–‡\n",
    "        \n",
    "        answers = examples[\"answers\"][sample_index] # å¯¹åº”åŸæ–‡çš„ç­”æ¡ˆ\n",
    "\n",
    "        if len(answers[\"answer_start\"]) == 0: # 'answers': {'text': [''], 'answer_start': []}}ï¼ŒåŸæ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆ\n",
    "            # å¼€å§‹ç»“æŸä½ç½®è®°å½•ä¸º 0\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # ç­”æ¡ˆåœ¨åŸæ–‡ä¸­çš„å¼€å§‹å’Œç»“æŸä½ç½®\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0 # tokenå¯¹åº”çš„contextå¼€å§‹çš„ä½ç½®\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1 \n",
    "\n",
    "            token_end_index = len(input_ids) - 1 # tokenå¯¹åº”çš„contextç»“æŸçš„ä½ç½®\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1 \n",
    "            \n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                # å½“å‰chunkä¸­æ²¡æœ‰ç­”æ¡ˆï¼ˆå¿…é¡»å®Œæ•´åŒ…å«ï¼‰\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # å½“å‰chunkä¸­æœ‰ç­”æ¡ˆ\n",
    "                # åç§»tokençš„ä½ç½®ï¼Œæ‰¾åˆ°ç­”æ¡ˆå¯¹åº”çš„tokenå¼€å§‹ä½ç½®\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                # å‰ç§»tokençš„ä½ç½®ï¼Œæ‰¾åˆ°ç­”æ¡ˆå¯¹åº”çš„tokenç»“æŸä½ç½®\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "228562ca-ab51-40ea-9b3c-e1a10657827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized_example = prepare_train_features(test_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8bc002a-61c7-4626-8830-283453d28a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93516915-82fb-4ce5-9ffd-26004c8fb892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 88524\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "        num_rows: 10784\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20eea042-bfe9-4513-ad5e-b125d8321db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0dce104-ba25-452c-9ee2-16c640e7d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=192\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08ea8ba8-b9c7-4692-b3e5-5f675c2c7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33871300-67b3-4de5-861d-20f497c11f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62867476-300f-4322-a5c0-ee2466d8f58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1386' max='1386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1386/1386 33:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.428168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.157600</td>\n",
       "      <td>1.275152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.303900</td>\n",
       "      <td>1.251565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(model_dir)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af3993d9-21aa-4ad2-80b0-3c88fa8048c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    \"\"\"\n",
    "    æ ¹æ®ç‰¹å¾å’Œé¢„æµ‹è¿˜åŸåˆ°åŸæœ¬çš„ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # æ—¥å¿—è®°å½•ã€‚\n",
    "    print(f\"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚\")\n",
    "\n",
    "    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚\n",
    "        for feature_index in feature_indices:\n",
    "            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ed87036-b760-4b89-9e2b-bbd15d388b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    \"\"\"\n",
    "    å°†éªŒè¯æ•°æ®å¤„ç†ä¸ºfeatures\n",
    "    \"\"\"\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚\n",
    "    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ\n",
    "    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "908b6add-fe30-4dae-9bd2-574f70cad299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcedf63e7b43424cb6b2138a9b01e73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ca15fb5d-623a-4c42-bf30-bc72dc5504b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
       "    num_rows: 10784\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a0b17eb9-5228-4052-9f9a-140cc094bab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)\n",
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# åŸºäºfeattureçš„åŸå§‹é¢„æµ‹ç»“æœ\n",
    "raw_predictions = trained_trainer.predict(validation_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a0f503b-6b31-45ee-b712-3eb7ff54c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=(array([[-5.286222 , -6.612807 , -7.293781 , ..., -7.5817404, -7.56017  ,\n",
      "        -7.627872 ],\n",
      "       [-5.298247 , -6.6009326, -7.273103 , ..., -7.583229 , -7.5616307,\n",
      "        -7.628161 ],\n",
      "       [-2.720223 , -5.06894  , -6.461087 , ..., -7.641131 , -7.598675 ,\n",
      "        -7.589619 ],\n",
      "       ...,\n",
      "       [-2.084156 , -6.447255 , -6.801171 , ..., -7.570975 , -7.555427 ,\n",
      "        -7.5553794],\n",
      "       [-1.9123228, -6.6626377, -6.757734 , ..., -7.5725675, -7.5949917,\n",
      "        -7.569608 ],\n",
      "       [-1.7271022, -6.8095   , -6.809823 , ..., -7.6145835, -7.5947175,\n",
      "        -7.5965524]], dtype=float32), array([[-4.779095 , -7.073431 , -6.561958 , ..., -7.4892554, -7.509979 ,\n",
      "        -7.4518743],\n",
      "       [-4.8729267, -7.0950127, -6.587563 , ..., -7.4872966, -7.5079274,\n",
      "        -7.4510503],\n",
      "       [-2.4264112, -6.2001505, -7.2404637, ..., -7.432796 , -7.484364 ,\n",
      "        -7.4749546],\n",
      "       ...,\n",
      "       [-2.0083191, -7.132052 , -7.415781 , ..., -7.5316133, -7.5426645,\n",
      "        -7.58057  ],\n",
      "       [-1.8068904, -7.087704 , -7.2247133, ..., -7.5359416, -7.4681125,\n",
      "        -7.5394278],\n",
      "       [-1.5354849, -6.892689 , -7.496103 , ..., -7.5043445, -7.524335 ,\n",
      "        -7.5558014]], dtype=float32)), label_ids=None, metrics={'test_runtime': 27.1306, 'test_samples_per_second': 397.485, 'test_steps_per_second': 2.101})\n",
      "10784\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(raw_predictions)\n",
    "print(len(raw_predictions.predictions[0]))\n",
    "print(len(raw_predictions.predictions[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71c32241-8d9a-4397-a234-8503f3fcde52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364658cab24f4bcba5e7bf4816fa77f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_900124/2594762423.py:10: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad\")\n",
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9c37c346b64d89846b406b8233965a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fb8fab4f9c456498ddfda5d7c7b572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 71.57048249763481, 'f1': 81.15865261806042}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# åŸºäºåŸæ•°æ®ã€ç‰¹å¾ã€é¢„æµ‹ç»“æœè¿˜åŸå›ç­”\n",
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)\n",
    "formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "\n",
    "# æ ‡å‡†ç­”æ¡ˆ\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "# è®¡ç®—å‡†ç¡®ç‡å’Œf1 score\n",
    "metric = load_metric(\"squad\")\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ab85e-4502-48a5-9adb-c249fe3b65cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
