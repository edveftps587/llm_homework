{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bbf0289-e177-4160-a1fe-9ad6c78ff986",
   "metadata": {},
   "source": [
    "## åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ec832c-33cc-4f6e-b01a-62927e7c3093",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'THUDM/chatglm3-6b-128k'\n",
    "\n",
    "# Lora ç›¸å…³çš„è¶…å‚æ•°\n",
    "lora_rank = 64                     \n",
    "lora_alpha = 64                    \n",
    "lora_dropout = 0.1                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6cc232-204f-4dcb-892d-118bd1ded0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, \n",
    "                                          trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53f4321-7cce-496e-ad7e-83f969fb0170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9dddd354b5f4defa6b4b81926a22713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "_compute_dtype_map = {\n",
    "    'fp32': torch.float32,\n",
    "    'fp16': torch.float16,\n",
    "    'bf16': torch.bfloat16\n",
    "}\n",
    "\n",
    "# QLoRA é‡åŒ–é…ç½®\n",
    "q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                              bnb_4bit_quant_type='nf4',\n",
    "                              bnb_4bit_use_double_quant=True,\n",
    "                              bnb_4bit_compute_dtype=_compute_dtype_map['bf16'])\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                            quantization_config=q_config,\n",
    "                                            device_map='auto',\n",
    "                                            trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1efeace-7363-4ad2-b17b-4ec38a401bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€ä¸ªåä¸ºæ™ºè°±æ¸…è¨€çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œæ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œæ™ºè°± AI å…¬å¸äº 2023 å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¼€å‘çš„ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚\n",
      "[{'role': 'user', 'content': 'è¯·é—®ä½ æ˜¯è°ï¼Ÿ'}, {'role': 'assistant', 'metadata': '', 'content': 'æˆ‘æ˜¯ä¸€ä¸ªåä¸ºæ™ºè°±æ¸…è¨€çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œæ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œæ™ºè°± AI å…¬å¸äº 2023 å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¼€å‘çš„ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚'}]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"è¯·é—®ä½ æ˜¯è°ï¼Ÿ\"\n",
    "response, history = model.chat(tokenizer=tokenizer, query=input_text, history=[])\n",
    "print(response)\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18adb8c9-6455-437d-bd5d-b9b0de9b3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "from peft import TaskType, LoraConfig\n",
    "\n",
    "target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING['chatglm']\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=target_modules,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2040c43c-385a-43d8-b372-09d2fbe9c753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "kbit_model = prepare_model_for_kbit_training(model)\n",
    "lora_model = get_peft_model(kbit_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb6013f0-e3c7-4b1f-9f5f-4e47b365f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15,597,568 || all params: 6,259,181,568 || trainable%: 0.24919500785441986\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ca368-d8a4-4f8a-84e7-959a4b4d22dd",
   "metadata": {},
   "source": [
    "# æ•°æ®å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea01393-60f8-43b5-a6f5-951cc7247b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['content', 'summary'],\n",
      "    num_rows: 1101\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "excel_file_path = 'data/chatglm-summary-0310.xlsx'\n",
    "df = pd.read_excel(excel_file_path)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b1329f-3c5b-408e-afb6-ec43de242fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç‰¹å¾ 'content' çš„æœ€å¤§é•¿åº¦: 32767, å¯¹åº”çš„å€¼: è¿™æ—¶ï¼Œå¥¹å·²çŸ¥é“çˆ¶äº²æ—¶æ—¶å¾€ä¼¦æ•¦æ˜¯ä¸ºç€ä»€ä¹ˆã€‚\n",
      "   \n",
      "\n",
      "    æœ‰ä¸€ä¸ªäººåœ¨é‚£ä¸ªåŸå¸‚ã€‚\n",
      "   \n",
      "\n",
      "    è€Œä¸”ï¼Œé‚£äººé€æ¸åš£å¼ ï¼Œç”µè¯ç”µè®¯æ—¶æ—¶ä¼ åˆ°èƒ¡å®¶ã€‚\n",
      "   \n",
      "\n",
      "    ä¸€æ—¥ï¼Œç›´å­å‘Šè¯‰èƒ¡çƒï¼šâ€œæˆ‘å°†éšå‘å…ˆç”Ÿåˆ°ä¼¦\n",
      "----------\n",
      "ç‰¹å¾ 'summary' çš„æœ€å¤§é•¿åº¦: 948, å¯¹åº”çš„å€¼: æ³•å›½ç°ä»£æ–‡å­¦å²ä¸Šçš„ä¸€äº›é‡è¦ä½œå®¶åŒ…æ‹¬å¡ç»´å°¼å¤«äººã€åšé©¬èˆã€å¤§ä»²é©¬ã€æ¢…é‡Œç¾ã€ä¹”æ²»Â·æ¡‘ã€ç¦æ¥¼æ‹œã€é¾šå¤å°”å…„å¼Ÿã€å‡¡å°”çº³ã€å·¦æ‹‰ã€éƒ½å¾·ã€æ³•æœ—å£«ã€å…°æ³¢ã€ç½—å…°ã€çºªå¾·ã€æ™®é²æ–¯ç‰¹ã€è«æ³Šæ¡‘ã€è«æ´›äºšã€æœ±æ—Â·æ ¼æ—ã€è¨ç‰¹ã€åŠ ç¼ªç­‰ã€‚è¿™\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸æ¥ä¿å­˜æ¯ä¸€åˆ—çš„æœ€å¤§é•¿åº¦å’Œå¯¹åº”çš„å€¼\n",
    "max_length_values = {feature: {\"max_length\": 0, \"value\": \"\"} for feature in dataset.features}\n",
    "\n",
    "# éå†æ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œ\n",
    "for row in dataset:\n",
    "    # å¯¹äºæ¯ä¸€åˆ—ï¼Œæ›´æ–°æœ€å¤§é•¿åº¦å’Œå¯¹åº”çš„å€¼\n",
    "    for feature in dataset.features:\n",
    "        current_length = len(row[feature])\n",
    "        if current_length > max_length_values[feature][\"max_length\"]:\n",
    "            max_length_values[feature][\"max_length\"] = current_length\n",
    "            max_length_values[feature][\"value\"] = row[feature]\n",
    "\n",
    "# æ‰“å°æ¯ä¸€åˆ—çš„æœ€å¤§é•¿åº¦å’Œå¯¹åº”çš„å€¼\n",
    "for feature, info in max_length_values.items():\n",
    "    print(f\"ç‰¹å¾ '{feature}' çš„æœ€å¤§é•¿åº¦: {info['max_length']}, å¯¹åº”çš„å€¼: {info['value'][:100]}\")\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ae11f1-73d1-4411-a61a-23d18d4c2d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21308\n",
      "658\n"
     ]
    }
   ],
   "source": [
    "# ç¡®å®šä¸‹æœ€å¤§é•¿åº¦\n",
    "q_ids = tokenizer.encode(text=max_length_values['content']['value'], add_special_tokens=False)\n",
    "a_ids = tokenizer.encode(text=max_length_values['summary']['value'], add_special_tokens=False)\n",
    "print(len(q_ids))\n",
    "print(len(a_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d38cab4a-395f-4e7f-8a8e-7619902e0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 22000                    \n",
    "max_output_length = 800\n",
    "prompt_text = \"\"\"ä»¥ä¸‹æ˜¯æ¥è‡ªäºæŸæœ¬ä¹¦ä¸­çš„å•ç« èŠ‚å†…å®¹ï¼Œè¯·ä½ ä½¿ç”¨ã€Œå¹³é“ºç›´å™ã€çš„æ–¹å¼æ€»ç»“ä¸‹æœ¬ç« èŠ‚çš„å†…å®¹\n",
    "---------ä»¥ä¸‹æ˜¯ç« èŠ‚å†…å®¹---------\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d93662-86d7-473a-9c48-89fe41f4076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(example, tokenizer, ignore_label_id=-100):\n",
    "    \"\"\"\n",
    "    å¯¹å•ä¸ªæ•°æ®æ ·æœ¬è¿›è¡Œtokenizeå¤„ç†ã€‚\n",
    "\n",
    "    å‚æ•°:\n",
    "    example (dict): åŒ…å«'content'å’Œ'summary'é”®çš„å­—å…¸ï¼Œä»£è¡¨è®­ç»ƒæ•°æ®çš„ä¸€ä¸ªæ ·æœ¬ã€‚\n",
    "    tokenizer (transformers.PreTrainedTokenizer): ç”¨äºtokenizeæ–‡æœ¬çš„tokenizerã€‚\n",
    "    ignore_label_id (int, optional): åœ¨labelä¸­ç”¨äºå¡«å……çš„å¿½ç•¥IDï¼Œé»˜è®¤ä¸º-100ã€‚\n",
    "\n",
    "    è¿”å›:\n",
    "    dict: åŒ…å«'tokenized_input_ids'å’Œ'labels'çš„å­—å…¸ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    # æ„å»ºé—®é¢˜æ–‡æœ¬\n",
    "    example['content'] = example['content'].replace('\\n', '')\n",
    "    question = prompt_text + example['content']\n",
    "    if example.get('input', None) and example['input'].strip():\n",
    "        question += f'\\n{example[\"input\"]}'\n",
    "\n",
    "    # æ„å»ºç­”æ¡ˆæ–‡æœ¬\n",
    "    answer = example['summary']\n",
    "\n",
    "    # å¯¹é—®é¢˜å’Œç­”æ¡ˆæ–‡æœ¬è¿›è¡Œtokenizeå¤„ç†\n",
    "    q_ids = tokenizer.encode(text=question, add_special_tokens=False)\n",
    "    a_ids = tokenizer.encode(text=answer, add_special_tokens=False)\n",
    "\n",
    "    # å¦‚æœtokenizeåçš„é•¿åº¦è¶…è¿‡æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œåˆ™è¿›è¡Œæˆªæ–­\n",
    "    if len(q_ids) > max_input_length - 2:  # ä¿ç•™ç©ºé—´ç»™gmaskå’Œbosæ ‡è®°\n",
    "        q_ids = q_ids[:max_input_length - 2]\n",
    "    if len(a_ids) > max_output_length - 1:  # ä¿ç•™ç©ºé—´ç»™eosæ ‡è®°\n",
    "        a_ids = a_ids[:max_output_length - 1]\n",
    "\n",
    "    # æ„å»ºæ¨¡å‹çš„è¾“å…¥æ ¼å¼\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(q_ids, a_ids)\n",
    "    question_length = len(q_ids) + 2  # åŠ ä¸Šgmaskå’Œbosæ ‡è®°\n",
    "\n",
    "    # æ„å»ºæ ‡ç­¾ï¼Œå¯¹äºé—®é¢˜éƒ¨åˆ†çš„è¾“å…¥ä½¿ç”¨ignore_label_idè¿›è¡Œå¡«å……\n",
    "    labels = [ignore_label_id] * question_length + input_ids[question_length:]\n",
    "\n",
    "    return {'input_ids': input_ids, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff22fe0a-142d-46ca-acbf-b63d2da48c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94c93c422fc40faa7148324669ded08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_names = dataset.column_names\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda example: tokenize_func(example, tokenizer),\n",
    "    batched=False, \n",
    "    remove_columns=column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5b2ec9c-0b49-4a1f-9014-884d86954b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 1101\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77953152-cf99-44ed-97ed-d1e071da9932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f069f-919d-479f-9eb0-d0767f7fbfce",
   "metadata": {},
   "source": [
    "# å‡†å¤‡è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1dd2ecc-f9a3-4bd4-8afa-5da9a8112e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# DataCollatorForChatGLM ç±»\n",
    "class DataCollatorForChatGLM:\n",
    "    \"\"\"\n",
    "    ç”¨äºå¤„ç†æ‰¹é‡æ•°æ®çš„DataCollatorï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ ChatGLM æ¨¡å‹æ—¶ã€‚\n",
    "\n",
    "    è¯¥ç±»è´Ÿè´£å°†å¤šä¸ªæ•°æ®æ ·æœ¬ï¼ˆtokenized inputï¼‰åˆå¹¶ä¸ºä¸€ä¸ªæ‰¹é‡ï¼Œå¹¶åœ¨å¿…è¦æ—¶è¿›è¡Œå¡«å……(padding)ã€‚\n",
    "\n",
    "    å±æ€§:\n",
    "    pad_token_id (int): ç”¨äºå¡«å……(padding)çš„token IDã€‚\n",
    "    max_length (int): å•ä¸ªæ‰¹é‡æ•°æ®çš„æœ€å¤§é•¿åº¦é™åˆ¶ã€‚\n",
    "    ignore_label_id (int): åœ¨æ ‡ç­¾ä¸­ç”¨äºå¡«å……çš„IDã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pad_token_id: int, max_length: int = max_output_length + max_input_length, ignore_label_id: int = -100):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–DataCollatorã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "        pad_token_id (int): ç”¨äºå¡«å……(padding)çš„token IDã€‚\n",
    "        max_length (int): å•ä¸ªæ‰¹é‡æ•°æ®çš„æœ€å¤§é•¿åº¦é™åˆ¶ã€‚\n",
    "        ignore_label_id (int): åœ¨æ ‡ç­¾ä¸­ç”¨äºå¡«å……çš„IDï¼Œé»˜è®¤ä¸º-100ã€‚\n",
    "        \"\"\"\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.ignore_label_id = ignore_label_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch_data: List[Dict[str, List]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        å¤„ç†æ‰¹é‡æ•°æ®ã€‚\n",
    "\n",
    "        å‚æ•°:\n",
    "        batch_data (List[Dict[str, List]]): åŒ…å«å¤šä¸ªæ ·æœ¬çš„å­—å…¸åˆ—è¡¨ã€‚\n",
    "\n",
    "        è¿”å›:\n",
    "        Dict[str, torch.Tensor]: åŒ…å«å¤„ç†åçš„æ‰¹é‡æ•°æ®çš„å­—å…¸ã€‚\n",
    "        \"\"\"\n",
    "        # è®¡ç®—æ‰¹é‡ä¸­æ¯ä¸ªæ ·æœ¬çš„é•¿åº¦\n",
    "        len_list = [len(d['input_ids']) for d in batch_data]\n",
    "        batch_max_len = max(len_list)  # æ‰¾åˆ°æœ€é•¿çš„æ ·æœ¬é•¿åº¦\n",
    "\n",
    "        input_ids, labels = [], []\n",
    "        for len_of_d, d in sorted(zip(len_list, batch_data), key=lambda x: -x[0]):\n",
    "            pad_len = batch_max_len - len_of_d  # è®¡ç®—éœ€è¦å¡«å……çš„é•¿åº¦\n",
    "            # æ·»åŠ å¡«å……ï¼Œå¹¶ç¡®ä¿æ•°æ®é•¿åº¦ä¸è¶…è¿‡æœ€å¤§é•¿åº¦é™åˆ¶\n",
    "            ids = d['input_ids'] + [self.pad_token_id] * pad_len\n",
    "            label = d['labels'] + [self.ignore_label_id] * pad_len\n",
    "            if batch_max_len > self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "                label = label[:self.max_length]\n",
    "            input_ids.append(torch.LongTensor(ids))\n",
    "            labels.append(torch.LongTensor(label))\n",
    "\n",
    "        # å°†å¤„ç†åçš„æ•°æ®å †å æˆä¸€ä¸ªtensor\n",
    "        input_ids = torch.stack(input_ids)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return {'input_ids': input_ids, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba5a124b-963e-41e2-9e67-2528d35f58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForChatGLM(pad_token_id=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac4b748-3cb6-4923-be6c-2cea5ed00b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"models/{model_name_or_path}_0310\",          # è¾“å‡ºç›®å½•\n",
    "    per_device_train_batch_size=2,                     # æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹é‡å¤§å°\n",
    "    gradient_accumulation_steps=4,                     # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°\n",
    "    learning_rate=1e-5,                                # å­¦ä¹ ç‡\n",
    "    num_train_epochs=5,                                # è®­ç»ƒè½®æ•°\n",
    "    lr_scheduler_type=\"linear\",                        # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹\n",
    "    warmup_ratio=0.05,                                  # é¢„çƒ­æ¯”ä¾‹\n",
    "    logging_steps=50,                                 # æ—¥å¿—è®°å½•æ­¥æ•°\n",
    "    save_strategy=\"epoch\",                             # æ¨¡å‹ä¿å­˜ç­–ç•¥\n",
    "    optim=\"adamw_torch\",                               # ä¼˜åŒ–å™¨ç±»å‹\n",
    "    fp16=True,                                        # æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f41f946-46a5-4cff-aba0-7fe1fdd7661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "481e9595-d105-49f0-aae7-dd8f1364d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='685' max='685' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [685/685 5:11:58, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.898000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.790200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.661100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.533400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/da_Ops01/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=685, training_loss=1.6314939178689554, metrics={'train_runtime': 18749.1608, 'train_samples_per_second': 0.294, 'train_steps_per_second': 0.037, 'total_flos': 1.1261881945405686e+18, 'train_loss': 1.6314939178689554, 'epoch': 4.97})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "955a4b6e-d85d-46a2-a567-b100abaa78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(f\"models/{model_name_or_path}_0310\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e0dc8-126a-4e55-ba84-cd4dd9402fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
